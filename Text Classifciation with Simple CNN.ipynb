{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:08:51.930916Z",
     "start_time": "2024-09-19T14:08:49.083485Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import ast"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/_message.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/libprotobuf.31.dylib\n  Referenced from: <16A21DF4-CEBC-3A0D-827E-044DC8E4A8F9> /Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/_message.cpython-311-darwin.so\n  Reason: tried: '/Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/../../../../../libprotobuf.31.dylib' (no such file), '/Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/../../../../../libprotobuf.31.dylib' (no such file), '/Users/iangosling/anaconda3/bin/../lib/libprotobuf.31.dylib' (no such file), '/Users/iangosling/anaconda3/bin/../lib/libprotobuf.31.dylib' (no such file), '/usr/local/lib/libprotobuf.31.dylib' (no such file), '/usr/lib/libprotobuf.31.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SelectKBest\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m f_classif\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dense\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/__init__.py:45\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2 \u001B[38;5;28;01mas\u001B[39;00m _tf2\n\u001B[1;32m     43\u001B[0m _tf2\u001B[38;5;241m.\u001B[39menable()\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __operators__\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m audio\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autograph\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m decorator\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dispatch\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mag_ctx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m control_status_ctx \u001B[38;5;66;03m# line: 34\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimpl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf_convert\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mthreading\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ag_logging\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtf_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf_export\n\u001B[1;32m     25\u001B[0m stacks \u001B[38;5;241m=\u001B[39m threading\u001B[38;5;241m.\u001B[39mlocal()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext_managers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m control_dependency_on_returns\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m alias_tensors\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensor_list\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dynamic_list_append\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_array_ops\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontrol_dependency_on_returns\u001B[39m(return_value):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:33\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m typing \u001B[38;5;28;01mas\u001B[39;00m npt\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m message\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m attr_value_pb2\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m full_type_pb2\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m function_pb2\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/core/framework/attr_value_pb2.py:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -*- coding: utf-8 -*-\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# source: tensorflow/core/framework/attr_value.proto\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minternal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m builder \u001B[38;5;28;01mas\u001B[39;00m _builder\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m descriptor \u001B[38;5;28;01mas\u001B[39;00m _descriptor\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m descriptor_pool \u001B[38;5;28;01mas\u001B[39;00m _descriptor_pool\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/google/protobuf/internal/builder.py:42\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minternal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m enum_type_wrapper\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m message \u001B[38;5;28;01mas\u001B[39;00m _message\n\u001B[0;32m---> 42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m reflection \u001B[38;5;28;01mas\u001B[39;00m _reflection\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m symbol_database \u001B[38;5;28;01mas\u001B[39;00m _symbol_database\n\u001B[1;32m     45\u001B[0m _sym_db \u001B[38;5;241m=\u001B[39m _symbol_database\u001B[38;5;241m.\u001B[39mDefault()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/google/protobuf/reflection.py:51\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;03m\"\"\"Contains a metaclass and helper functions used to create\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;03mprotocol message classes from Descriptor objects at runtime.\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03mthis file*.\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     48\u001B[0m __author__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrobinson@google.com (Will Robinson)\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m message_factory\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m symbol_database\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# The type of all Message classes.\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Part of the public interface, but normally only used by message factories.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/google/protobuf/message_factory.py:43\u001B[0m\n\u001B[1;32m     40\u001B[0m __author__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatthewtoia@google.com (Matt Toia)\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minternal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m api_implementation\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m descriptor_pool\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m message\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m api_implementation\u001B[38;5;241m.\u001B[39mType() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpp\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/google/protobuf/descriptor_pool.py:63\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m descriptor\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m descriptor_database\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m text_encoding\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/google/protobuf/descriptor.py:47\u001B[0m\n\u001B[1;32m     45\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbinascii\u001B[39;00m\n\u001B[1;32m     46\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m   \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _message\n\u001B[1;32m     48\u001B[0m   _USE_C_DESCRIPTORS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mError\u001B[39;00m(\u001B[38;5;167;01mException\u001B[39;00m):\n",
      "\u001B[0;31mImportError\u001B[0m: dlopen(/Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/_message.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/libprotobuf.31.dylib\n  Referenced from: <16A21DF4-CEBC-3A0D-827E-044DC8E4A8F9> /Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/_message.cpython-311-darwin.so\n  Reason: tried: '/Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/../../../../../libprotobuf.31.dylib' (no such file), '/Users/iangosling/anaconda3/lib/python3.11/site-packages/google/protobuf/pyext/../../../../../libprotobuf.31.dylib' (no such file), '/Users/iangosling/anaconda3/bin/../lib/libprotobuf.31.dylib' (no such file), '/Users/iangosling/anaconda3/bin/../lib/libprotobuf.31.dylib' (no such file), '/usr/local/lib/libprotobuf.31.dylib' (no such file), '/usr/lib/libprotobuf.31.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:56:05.035802Z",
     "start_time": "2024-09-19T13:56:05.030858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ],
   "id": "501ab40700007e06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/iangosling/anaconda3/bin/python\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T14:08:14.735403Z",
     "start_time": "2024-09-19T14:08:14.576636Z"
    }
   },
   "cell_type": "code",
   "source": "print(tf.__version__)",
   "id": "ef908e4aeb21fa4c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtf\u001B[49m\u001B[38;5;241m.\u001B[39m__version__)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T14:01:28.229640Z",
     "start_time": "2024-09-19T14:01:25.524738Z"
    }
   },
   "cell_type": "code",
   "source": "pip install tensorflow-macos",
   "id": "a25f17763f6c1130",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-macos in /Users/iangosling/anaconda3/lib/python3.11/site-packages (2.16.2)\r\n",
      "Requirement already satisfied: tensorflow==2.16.2 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow-macos) (2.16.2)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.11.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.3.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.20.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (68.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.11.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.14.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.64.1)\r\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.16.2)\r\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.5.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.37.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.24.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2->tensorflow-macos) (0.35.1)\r\n",
      "Requirement already satisfied: rich in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (13.7.1)\r\n",
      "Requirement already satisfied: namex in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.12.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.4.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.0)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/iangosling/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Most of this code comes from Googles Training course on text classification:  https://developers.google.com/machine-learning/guides/text-classification",
   "id": "e154e6a10cc0a733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1 - Import Train and Test Data and convert to required data type",
   "id": "d68819a701c9ab8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:11:32.487534Z",
     "start_time": "2024-09-19T08:11:32.188862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load training set from CSV, convert CSV text to list and join into a single string\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "train_data['preprocessed_text'] = train_data['preprocessed_text'].apply(ast.literal_eval)\n",
    "train_data['preprocessed_text'] = train_data['preprocessed_text'].apply(' '.join)\n",
    "train_texts = train_data['preprocessed_text'].to_list()\n",
    "train_labels = train_data['clause_type'].to_numpy()\n",
    "\n",
    "# Load test set from CSV, convert CSV text to list and join into a single string\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "test_data['preprocessed_text'] = test_data['preprocessed_text'].apply(ast.literal_eval)\n",
    "test_data['preprocessed_text'] = test_data['preprocessed_text'].apply(' '.join)\n",
    "val_texts = test_data['preprocessed_text'].to_list()\n",
    "val_labels = test_data['clause_type'].to_numpy()\n"
   ],
   "id": "e9682e5da79e13db",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:34:39.123724Z",
     "start_time": "2024-09-19T08:34:39.119182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Element: {u}, Count: {c}\")"
   ],
   "id": "323c43d2c44a9af9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: arbitration, Count: 178\n",
      "Element: capitalization, Count: 146\n",
      "Element: confidential-information, Count: 182\n",
      "Element: confidentiality, Count: 159\n",
      "Element: contribution, Count: 132\n",
      "Element: indemnification, Count: 151\n",
      "Element: indemnification-and-contribution, Count: 140\n",
      "Element: indemnification-by-the-company, Count: 184\n",
      "Element: participations, Count: 159\n",
      "Element: payment-of-expenses, Count: 181\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:34:03.030166Z",
     "start_time": "2024-09-19T08:34:03.024479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique, counts = np.unique(val_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Element: {u}, Count: {c}\")"
   ],
   "id": "f4927272c886d1bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: arbitration, Count: 62\n",
      "Element: capitalization, Count: 54\n",
      "Element: confidential-information, Count: 58\n",
      "Element: confidentiality, Count: 61\n",
      "Element: contribution, Count: 48\n",
      "Element: indemnification, Count: 59\n",
      "Element: indemnification-and-contribution, Count: 40\n",
      "Element: indemnification-by-the-company, Count: 46\n",
      "Element: participations, Count: 51\n",
      "Element: payment-of-expenses, Count: 59\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:57:10.616695Z",
     "start_time": "2024-09-19T08:57:10.547188Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "530b614695f179bf",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m get_num_classes(train_labels)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(num_classes)\n",
      "Cell \u001B[0;32mIn[64], line 16\u001B[0m, in \u001B[0;36mget_num_classes\u001B[0;34m(labels)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_num_classes\u001B[39m(labels):\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Gets the total number of classes.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    # Arguments\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124;03m            is missing or if number of classes is <= 1.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(labels) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     17\u001B[0m     missing_classes \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_classes) \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m labels]\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_classes):\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2 - Function for Vectorisation",
   "id": "6f2269a67ee43193"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:12:11.338548Z",
     "start_time": "2024-09-19T08:12:11.331782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "# Whether text should be split into word or character n-grams. One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts).astype(np.float32)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts).astype(np.float32)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ],
   "id": "c87db60adab68992",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b0b7ace1421b495"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3 - Define Function to create last layer according to data parameters",
   "id": "ba754b00a763e7ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T15:30:47.545634Z",
     "start_time": "2024-09-18T15:30:47.538522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ],
   "id": "fb8ad1b9131b9f48",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4 - Define function to build simple multi-layer perceptron  ",
   "id": "ed7765d66908b863"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T15:41:25.337588Z",
     "start_time": "2024-09-18T15:41:25.333710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ],
   "id": "df68b7eeb4ef9a6e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 5 - Create a function to train the n_gram model",
   "id": "36ea8c1fa496035b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:43:54.095860Z",
     "start_time": "2024-09-19T13:43:54.080842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "        # Vectorize texts.\n",
    "    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                      units=units,\n",
    "                      dropout_rate=dropout_rate,\n",
    "                      input_shape=x_train.shape[1:],\n",
    "                      num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('IMDb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ],
   "id": "abd70653b3f68254",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 6 - Vectorise data",
   "id": "aa5c5631d63c4fac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:41:15.496984Z",
     "start_time": "2024-09-19T13:41:15.159987Z"
    }
   },
   "cell_type": "code",
   "source": "ngram_vectorize(train_texts, train_labels, val_texts)",
   "id": "8098012f6642dfcc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iangosling/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2065: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<1612x17581 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 233016 stored elements in Compressed Sparse Row format>,\n",
       " <538x17581 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 71621 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 7 - Build Model and train model",
   "id": "51a79aca1c33a7e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:41:19.091466Z",
     "start_time": "2024-09-19T13:41:19.089292Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eb3d3700ecd5b571",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:43:26.563358Z",
     "start_time": "2024-09-19T13:43:26.155290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = 10\n",
    "data = ((train_texts, train_labels), (val_texts, val_labels))\n",
    "train_ngram_model(data)"
   ],
   "id": "9cd6af2e41d3676f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iangosling/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2065: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.optimizers.adam.Adam object at 0x16e23bed0>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[116], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m ((train_texts, train_labels), (val_texts, val_labels))\n\u001B[0;32m----> 3\u001B[0m train_ngram_model(data)\n",
      "Cell \u001B[0;32mIn[115], line 38\u001B[0m, in \u001B[0;36mtrain_ngram_model\u001B[0;34m(data, learning_rate, epochs, batch_size, layers, units, dropout_rate)\u001B[0m\n\u001B[1;32m     36\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     37\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate)\n\u001B[0;32m---> 38\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer, loss\u001B[38;5;241m=\u001B[39mloss, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124macc\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Create callback for early stopping on validation loss. If the loss does\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# not decrease in two consecutive tries, stop training.\u001B[39;00m\n\u001B[1;32m     42\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m [tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mEarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/engine/training.py:568\u001B[0m, in \u001B[0;36mModel.compile\u001B[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_compile(optimizer, metrics, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_eagerly \u001B[38;5;241m=\u001B[39m run_eagerly\n\u001B[0;32m--> 568\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_optimizer(optimizer)\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_loss \u001B[38;5;241m=\u001B[39m compile_utils\u001B[38;5;241m.\u001B[39mLossesContainer(\n\u001B[1;32m    570\u001B[0m     loss, loss_weights, output_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_names)\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_metrics \u001B[38;5;241m=\u001B[39m compile_utils\u001B[38;5;241m.\u001B[39mMetricsContainer(\n\u001B[1;32m    572\u001B[0m     metrics, weighted_metrics, output_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_names,\n\u001B[1;32m    573\u001B[0m     from_serialized\u001B[38;5;241m=\u001B[39mfrom_serialized)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/engine/training.py:606\u001B[0m, in \u001B[0;36mModel._get_optimizer\u001B[0;34m(self, optimizer)\u001B[0m\n\u001B[1;32m    603\u001B[0m       opt \u001B[38;5;241m=\u001B[39m lso\u001B[38;5;241m.\u001B[39mLossScaleOptimizerV1(opt, loss_scale)\n\u001B[1;32m    604\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m opt\n\u001B[0;32m--> 606\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nest\u001B[38;5;241m.\u001B[39mmap_structure(_get_single_optimizer, optimizer)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001B[0m, in \u001B[0;36mmap_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[0;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[1;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[0;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[1;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/engine/training.py:597\u001B[0m, in \u001B[0;36mModel._get_optimizer.<locals>._get_single_optimizer\u001B[0;34m(opt)\u001B[0m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_single_optimizer\u001B[39m(opt):\n\u001B[0;32m--> 597\u001B[0m   opt \u001B[38;5;241m=\u001B[39m optimizers\u001B[38;5;241m.\u001B[39mget(opt)\n\u001B[1;32m    598\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (loss_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    599\u001B[0m       \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(opt, lso\u001B[38;5;241m.\u001B[39mLossScaleOptimizer)):\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m loss_scale \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdynamic\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/optimizers.py:131\u001B[0m, in \u001B[0;36mget\u001B[0;34m(identifier)\u001B[0m\n\u001B[1;32m    129\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m deserialize(config)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 131\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    132\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCould not interpret optimizer identifier: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(identifier))\n",
      "\u001B[0;31mValueError\u001B[0m: Could not interpret optimizer identifier: <keras.optimizers.adam.Adam object at 0x16e23bed0>"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d52ae0eb52a0a4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e83184e15dc425f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "faf2a6d13f61feca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 8 - Tune Hyperparameters",
   "id": "bc2e6b6ffef036ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "47dbf769c906ace2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ece1420b3640de4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 10 - Test Model\n",
   "id": "6971e517bc25e782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1a44ea0e700efa0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
