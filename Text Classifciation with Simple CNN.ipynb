{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-19T10:11:24.925676Z",
     "start_time": "2024-09-19T10:11:24.904020Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef908e4aeb21fa4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Most of this code comes from Googles Training course on text classification:  https://developers.google.com/machine-learning/guides/text-classification",
   "id": "e154e6a10cc0a733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1 - Import Train and Test Data and convert to required data type",
   "id": "d68819a701c9ab8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:11:32.487534Z",
     "start_time": "2024-09-19T08:11:32.188862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load training set from CSV, convert CSV text to list and join into a single string\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "train_data['preprocessed_text'] = train_data['preprocessed_text'].apply(ast.literal_eval)\n",
    "train_data['preprocessed_text'] = train_data['preprocessed_text'].apply(' '.join)\n",
    "train_texts = train_data['preprocessed_text'].to_list()\n",
    "train_labels = train_data['clause_type'].to_numpy()\n",
    "\n",
    "# Load test set from CSV, convert CSV text to list and join into a single string\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "test_data['preprocessed_text'] = test_data['preprocessed_text'].apply(ast.literal_eval)\n",
    "test_data['preprocessed_text'] = test_data['preprocessed_text'].apply(' '.join)\n",
    "val_texts = test_data['preprocessed_text'].to_list()\n",
    "val_labels = test_data['clause_type'].to_numpy()\n"
   ],
   "id": "e9682e5da79e13db",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:34:39.123724Z",
     "start_time": "2024-09-19T08:34:39.119182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Element: {u}, Count: {c}\")"
   ],
   "id": "323c43d2c44a9af9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: arbitration, Count: 178\n",
      "Element: capitalization, Count: 146\n",
      "Element: confidential-information, Count: 182\n",
      "Element: confidentiality, Count: 159\n",
      "Element: contribution, Count: 132\n",
      "Element: indemnification, Count: 151\n",
      "Element: indemnification-and-contribution, Count: 140\n",
      "Element: indemnification-by-the-company, Count: 184\n",
      "Element: participations, Count: 159\n",
      "Element: payment-of-expenses, Count: 181\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:34:03.030166Z",
     "start_time": "2024-09-19T08:34:03.024479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique, counts = np.unique(val_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Element: {u}, Count: {c}\")"
   ],
   "id": "f4927272c886d1bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: arbitration, Count: 62\n",
      "Element: capitalization, Count: 54\n",
      "Element: confidential-information, Count: 58\n",
      "Element: confidentiality, Count: 61\n",
      "Element: contribution, Count: 48\n",
      "Element: indemnification, Count: 59\n",
      "Element: indemnification-and-contribution, Count: 40\n",
      "Element: indemnification-by-the-company, Count: 46\n",
      "Element: participations, Count: 51\n",
      "Element: payment-of-expenses, Count: 59\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:57:10.616695Z",
     "start_time": "2024-09-19T08:57:10.547188Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "530b614695f179bf",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m get_num_classes(train_labels)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(num_classes)\n",
      "Cell \u001B[0;32mIn[64], line 16\u001B[0m, in \u001B[0;36mget_num_classes\u001B[0;34m(labels)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_num_classes\u001B[39m(labels):\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Gets the total number of classes.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    # Arguments\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124;03m            is missing or if number of classes is <= 1.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(labels) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     17\u001B[0m     missing_classes \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_classes) \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m labels]\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_classes):\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2 - Function for Vectorisation",
   "id": "6f2269a67ee43193"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:12:11.338548Z",
     "start_time": "2024-09-19T08:12:11.331782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "# Whether text should be split into word or character n-grams. One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts).astype(np.float32)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts).astype(np.float32)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ],
   "id": "c87db60adab68992",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b0b7ace1421b495"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3 - Define Function to create last layer according to data parameters",
   "id": "ba754b00a763e7ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T15:30:47.545634Z",
     "start_time": "2024-09-18T15:30:47.538522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ],
   "id": "fb8ad1b9131b9f48",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4 - Define function to build simple multi-layer perceptron  ",
   "id": "ed7765d66908b863"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T15:41:25.337588Z",
     "start_time": "2024-09-18T15:41:25.333710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ],
   "id": "df68b7eeb4ef9a6e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 5 - Create a function to train the n_gram model",
   "id": "36ea8c1fa496035b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:19:48.813654Z",
     "start_time": "2024-09-19T09:19:48.807030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "        # Vectorize texts.\n",
    "    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                      units=units,\n",
    "                      dropout_rate=dropout_rate,\n",
    "                      input_shape=x_train.shape[1:],\n",
    "                      num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('IMDb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ],
   "id": "abd70653b3f68254",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 6 - Vectorise data",
   "id": "aa5c5631d63c4fac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:19:55.183863Z",
     "start_time": "2024-09-19T09:19:54.941743Z"
    }
   },
   "cell_type": "code",
   "source": "ngram_vectorize(train_texts, train_labels, val_texts)",
   "id": "8098012f6642dfcc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iangosling/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2065: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<1612x17581 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 233016 stored elements in Compressed Sparse Row format>,\n",
       " <538x17581 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 71621 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 7 - Build Model and train model",
   "id": "51a79aca1c33a7e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:11:34.239512Z",
     "start_time": "2024-09-19T10:11:34.236408Z"
    }
   },
   "cell_type": "code",
   "source": "print(tf.__version__)",
   "id": "eb3d3700ecd5b571",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:19:57.966534Z",
     "start_time": "2024-09-19T09:19:57.646337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = 10\n",
    "data = ((train_texts, train_labels), (val_texts, val_labels))\n",
    "train_ngram_model(data)"
   ],
   "id": "9cd6af2e41d3676f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iangosling/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2065: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.optimizers.legacy.adam.Adam object at 0x16dcd8050>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[86], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m ((train_texts, train_labels), (val_texts, val_labels))\n\u001B[0;32m----> 3\u001B[0m train_ngram_model(data)\n",
      "Cell \u001B[0;32mIn[84], line 38\u001B[0m, in \u001B[0;36mtrain_ngram_model\u001B[0;34m(data, learning_rate, epochs, batch_size, layers, units, dropout_rate)\u001B[0m\n\u001B[1;32m     36\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     37\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mlegacy\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate)\n\u001B[0;32m---> 38\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer, loss\u001B[38;5;241m=\u001B[39mloss, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124macc\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Create callback for early stopping on validation loss. If the loss does\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# not decrease in two consecutive tries, stop training.\u001B[39;00m\n\u001B[1;32m     42\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m [tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mEarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/engine/training.py:568\u001B[0m, in \u001B[0;36mModel.compile\u001B[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_compile(optimizer, metrics, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_eagerly \u001B[38;5;241m=\u001B[39m run_eagerly\n\u001B[0;32m--> 568\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_optimizer(optimizer)\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_loss \u001B[38;5;241m=\u001B[39m compile_utils\u001B[38;5;241m.\u001B[39mLossesContainer(\n\u001B[1;32m    570\u001B[0m     loss, loss_weights, output_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_names)\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_metrics \u001B[38;5;241m=\u001B[39m compile_utils\u001B[38;5;241m.\u001B[39mMetricsContainer(\n\u001B[1;32m    572\u001B[0m     metrics, weighted_metrics, output_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_names,\n\u001B[1;32m    573\u001B[0m     from_serialized\u001B[38;5;241m=\u001B[39mfrom_serialized)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/engine/training.py:606\u001B[0m, in \u001B[0;36mModel._get_optimizer\u001B[0;34m(self, optimizer)\u001B[0m\n\u001B[1;32m    603\u001B[0m       opt \u001B[38;5;241m=\u001B[39m lso\u001B[38;5;241m.\u001B[39mLossScaleOptimizerV1(opt, loss_scale)\n\u001B[1;32m    604\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m opt\n\u001B[0;32m--> 606\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nest\u001B[38;5;241m.\u001B[39mmap_structure(_get_single_optimizer, optimizer)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001B[0m, in \u001B[0;36mmap_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[0;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[1;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[0;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[1;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/engine/training.py:597\u001B[0m, in \u001B[0;36mModel._get_optimizer.<locals>._get_single_optimizer\u001B[0;34m(opt)\u001B[0m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_single_optimizer\u001B[39m(opt):\n\u001B[0;32m--> 597\u001B[0m   opt \u001B[38;5;241m=\u001B[39m optimizers\u001B[38;5;241m.\u001B[39mget(opt)\n\u001B[1;32m    598\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (loss_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    599\u001B[0m       \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(opt, lso\u001B[38;5;241m.\u001B[39mLossScaleOptimizer)):\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m loss_scale \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdynamic\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/keras/optimizers.py:131\u001B[0m, in \u001B[0;36mget\u001B[0;34m(identifier)\u001B[0m\n\u001B[1;32m    129\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m deserialize(config)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 131\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    132\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCould not interpret optimizer identifier: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(identifier))\n",
      "\u001B[0;31mValueError\u001B[0m: Could not interpret optimizer identifier: <keras.optimizers.legacy.adam.Adam object at 0x16dcd8050>"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d52ae0eb52a0a4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e83184e15dc425f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "faf2a6d13f61feca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 8 - Tune Hyperparameters",
   "id": "bc2e6b6ffef036ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "47dbf769c906ace2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ece1420b3640de4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 10 - Test Model\n",
   "id": "6971e517bc25e782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1a44ea0e700efa0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
